---
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: inline
---

```{r, warning=FALSE, message=FALSE}
#install.packages("PreProcess")
#install.packages("rattle")
library(corrplot)
library(dplyr)
library(rpart)
library(randomForest)
library(caret)
library(CORElearn)
library(PreProcess)
library(MASS)
library(rattle)
library(pROC)

```

# 1 Exploration

```{r, echo=T}
test <- read.table(file="test.csv", sep=",", header=TRUE) 
train <- read.table(file="train.csv", sep=",", header=TRUE)

set.seed(100)

```

**Target feature is biodegradability.**

```{r, echo=T}
summary(test)
```

**How balanced is the target variable?**

Mean of target variable is 1.337 that is how we know there are more not ready biodegradable chemicals in our training dataset than ready biodegradable chemicals.

```{r, echo=T}
table(train$Class)
```

```{r, echo=T}
table_class <- (table(train$Class)) 
piepercent <- paste(round(100*table_class/sum(table_class), 2), "%") 
names(table_class) <- c("ready biodegradable", "not ready biodegradable") 
labels <- paste(names(table_class), piepercent) 
pie(table_class, labels = labels, main = "target variable", col=c("pink", "lightblue1")) 
```

**Are there any missing values present? If there are, choose a strategy that takes this into account.**

Instances with missing values only consist 7% of data and we decided that is insignificant and we removed them.

```{r, echo=T}
is.na(train)

train[!complete.cases(train),] 
# 81/1055

#remove instances where one of values is missing 
train <- na.omit(train)
#test <- na.omit(test)
```

First we normalized our data

```{r, echo=T}
process <- preProcess(train, method=c("range"))
train <- predict(process, train)
test <- predict(process, test)
```

**Most of your data is of the numeric type. Can you identify, by adopting exploratory analysis, whether some features are directly related to the target? What about feature pairs?**

```{r, echo=T}

target_corr = abs(cor(train[,names(train)])[names(train)[42],])
target_corr <- sort(target_corr, decreasing = TRUE)
target_corr_top8 <- target_corr[2: 9]


plot(target_corr_top8, xlab="features", ylab = "correlation", main="features with biggest correalton to the target", xaxt = "n")
axis(1, at = c(1,2,3,4,5,6,7,8), labels = names(target_corr_top8))
```

Correlation matrix

```{r, echo=T}
correlation_matrix <- cor(train)

corrplot(correlation_matrix, method = 'color')

findCorrelation(correlation_matrix, names=TRUE, cutoff=0.9)

```

Scatter matrix

```{r, echo=T}
colors = c("#00AFBB", "#E7B800")
pairs(train[c("V1", "V27", "V39", "V12", "V13", "Class")], pch=19, col=c(2, 3))
```

```{r}
train1 = train[train$Class == 0,]
train2 = train[train$Class == 1,]
median1 <- apply(train1, 2, median)
median2 <- apply(train2, 2, median)
diff <- sort(abs(median1 - median2))
print(diff)
```

Distribution

```{r, echo=T}
p1 <- hist(train[train$Class == 0,]$V1) 
p2 <- hist(train[train$Class == 1,]$V1)                     # centered at 4
plot( p1, col=rgb(0,0,1,1/4), xlab = "V1", ylab = "Count", main="Distribucija")  # first histogram
plot( p2, col=rgb(1,0,0,1/4), add=T)
```

Box plot for feature V1

```{r, echo=T}
boxplot(V1 ~ Class, train)
```

# 2 Modeling

Make target variable non numeric

```{r, echo=T}
train[train$Class == 0,]$Class <- "C0"
train[train$Class == 1,]$Class <- "C1"
test[test$Class == 0,]$Class <- "C0"
test[test$Class == 1,]$Class <- "C1"

train$Class <- as.character(train$Class)
train$Class <- as.factor(train$Class)

test$Class <- as.character(test$Class)
test$Class <- as.factor(test$Class)
```

**Try to construct new features from existing ones.**

We looked at attribute information and saw 3 attributes that looked similar:

V5: F04[C-N]: Frequency of C-N at topological distance 4 V11: F03[C-N]: Frequency of C-N at topological distance 3 V34: F02[C-N]: Frequency of C-N at topological distance 2

We decided to add a new feature that combines these three features and we added the sum of V5, V11 and V34.

```{r, echo=T}
train <- train %>% mutate(V42 = V5 + V11 + V34)
test <- test %>% mutate(V42 = V5 + V11 + V34)
```

***feature selection***

We used two feature selection methods ReliefFequalK and Information gain and from both we created subset of 15 best features.

```{r, echo=TRUE}
feature_selection1 <- sort(attrEval(Class ~ ., train, "ReliefFequalK"), decreasing = TRUE)
selected_features1 <- c(names(head(feature_selection1, 15)))
selected_features1 <- append(selected_features1, "Class")
subset1 = train[selected_features1]
```

```{r}
feature_selection2 <- sort(attrEval(Class ~ ., train, "InfGain"), decreasing = TRUE)
selected_features2 <- c(names(head(feature_selection2, 15)))
selected_features2 <- append(selected_features2, "Class")
subset2 = train[selected_features2]
```

***majority classifier***

```{r, echo=T}
majority.class <- names(which.max(table(train$Class)))
majority.class

sum(train$Class == majority.class) / length(train$Class)
```

***random classifier***

```{r, echo=T}
# Generate random predictions with equal probability for each class
predictions <- sample(c("C0", "C1"), size = nrow(train), replace = TRUE, prob = c(0.5, 0.5)) 
# Calculate the accuracy of the random classifier 
accuracy <- mean(predictions == train$Class)
accuracy
```

***decision tree***

Decision tree seemed like a good place to start. It is simple but insightful.

```{r, echo=T}
train_control <- trainControl(method = "repeatedcv", number = 5, repeats=10, classProbs=TRUE, savePredictions = TRUE, summaryFunction = prSummary)

# Train the model using the train function and the tuneGrid argument
train.dt <- train(Class ~ ., data = train, tuneLength = 50, 
          method = "rpart", metric = "AUC", trControl = train_control)
train.dt.score <- train.dt$results[1,]

#train.dt.roc <- roc(train$Class, predict(train.dt, train, type="prob")[,2])
fancyRpartPlot(train.dt$finalModel)

```

**feature selection with decision tree**

We used decision tree for out thid feature selection and also generated subset with 15 top features.

```{r, echo=T}
importance <- varImp(train.dt$finalModel, scale=FALSE)
importance_sorted <- arrange(importance, desc(Overall))

selected_features3 <- c(row.names(head(importance_sorted, 15)))
selected_features3 <- append(selected_features3, "Class")
subset3 = train[selected_features3]
```

**decision tree with subsets**

```{r, echo=T}

train.dtS3 <- train(Class ~ ., data = subset3, tuneLength = 50, method = "rpart", metric = "AUC", trControl = train_control)
train.dtS3.score <- train.dtS3$results[1,]

#ReliefFequalK
train.dtS1 <- train(Class ~ ., data = subset1, tuneLength = 50, method = "rpart", metric = "AUC", trControl = train_control)
train.dtS1.score <- train.dtS1$results[1,]

#infGain
train.dtS2 <- train(Class ~ ., data = subset2, tuneLength = 50, method = "rpart", metric = "AUC", trControl = train_control)
train.dtS2.score <- train.dtS2$results[1,]

```

***Random forest***

```{r, echo=T}
set.seed(100)
tuneGrid <- expand.grid(.mtry = c(sqrt(ncol(train))))

trControl = trainControl(method='repeatedcv', number = 5, repeats = 10)
train.rf <- train(Class ~ ., data = train,method="rf",trControl=train_control, 
                  metric='AUC', tuneGrid=tuneGrid)
train.rf.score <- train.rf$results[1,]

```

```{r, echo=T}
var_imp <- varImp(train.rf)
## Create a plot of variable importance
var_imp %>%
        ggplot(aes(x=reorder(variables, importance), y=importance)) + 
        geom_bar(stat='identity') + 
        coord_flip() + 
        xlab('features') +
        labs(title='Random forest feature importance') + 
        theme_minimal()
```

**random forest with subsets**

```{r, echo=T}

#ReliefFequalK
train.rfS1 <- train(Class ~ ., data = subset1, method = "rf", metric = "AUC", trControl = train_control, tuneGrid=tuneGrid)
train.rfS1.score <- train.rfS1$results[1,]

#infGain
train.rfS2 <- train(Class ~ ., data = subset2, method = "rf", metric = "AUC", trControl = train_control, tuneGrid=tuneGrid)
train.rfS2.score <- train.rfS2$results[1,]

train.rfS3 <- train(Class ~ ., data = subset3, method = "rf", metric = "AUC", trControl = train_control, tuneGrid=tuneGrid)
train.rfS3.score <- train.rfS3$results[1,]

```

***KNN***

```{r, echo=T, warning=FALSE,}
knitr::opts_chunk$set(warning = FALSE) 

train$Class = factor(train$Class)
bigK = 50
grid = expand.grid(k = c(1:bigK))

train.knn <- train(Class ~., method= "knn", data = train, 
                    trControl = train_control, metric="AUC",
                     tuneGrid = grid, preProcess = c("scale", "center"))

best_k = which.max(train.knn$results$AUC)
train.knn.score <- train.knn$results[best_k,]

qplot(1:bigK, train.knn$results$AUC, xlab = "k", ylab = "Performance(AUC)", geom = c("point","line"))



```

**KNN with subsets**

```{r, echo=T}

#ReliefFequalK
train.knnS1 <- train(Class ~ ., data = subset1, method = "knn", metric = "AUC", trControl = train_control, tuneGrid=grid, preProcess = c("scale", "center"))
best_k = which.max(train.knnS1$results$AUC)
train.knnS1.score <- train.knnS1$results[best_k,]

#infGain
train.knnS2 <- train(Class ~ ., data = subset2, method = "knn", metric = "AUC", trControl = train_control, tuneGrid=grid, preProcess = c("scale", "center"))
best_k = which.max(train.knnS2$results$AUC)
train.knnS2.score <- train.knnS2$results[best_k,]

train.knnS3 <- train(Class ~ ., data = subset3, method = "knn", metric = "AUC", trControl = train_control, tuneGrid=grid, preProcess = c("scale", "center"))
best_k = which.max(train.knnS3$results$AUC)
train.knnS3.score <- train.knnS3$results[best_k,]

```

***LDA classifier***

```{r, echo=T, message=FALSE}
train.lda <- lda(Class~., train)
train.lda.values <- predict(train.lda, train)
ldahist(train.lda.values$x[,1], g=train$Class)
train.lda <-  train(Class ~., method= "lda", data = train,
                    trControl = train_control, metric="AUC",
                     preProcess = c("scale", "center"))
train.lda.score <- train.lda$results[1,]
```

***XGBoost***

```{r, echo = T, message=FALSE}
tune_grid <-  expand.grid(max_depth = c(3, 5, 7),
                        nrounds = (1:10)*50,    # number of trees
                        # default values below
                        eta = 0.3,
                        gamma = 0,
                        subsample = 1,
                        min_child_weight = 1,
                        colsample_bytree = 0.6)



train.xgboost <- train(Class ~ . , data = train, method = "xgbTree",
                trControl=train_control,
                tuneGrid = tune_grid,
                tuneLength = 10,
                verbosity = 0)
best_k = which.max(train.xgboost$results$AUC)
train.xgboost.score <- train.xgboost$results[best_k,]
```

**XGBoost with subsets**

```{r, echo=T, message=FALSE}

#ReliefFequalK
train.xgboostS1 <- train(Class ~ ., data = subset1, method = "xgbTree", metric = "AUC", trControl = train_control, tuneGrid=tune_grid, tuneLength = 10,verbosity = 0)
train.xgboostS1.score <- train.xgboostS1$results[1,]

#infGain
train.xgboostS2 <- train(Class ~ ., data = subset2, method = "xgbTree", metric = "AUC", trControl = train_control, tuneGrid=tune_grid, tuneLength = 10,verbosity = 0)
train.xgboostS2.score <- train.xgboostS2$results[1,]

train.xgboostS3 <- train(Class ~ ., data = subset3, method = "xgbTree", metric = "AUC", trControl = train_control, tuneGrid=tune_grid, tuneLength = 10,verbosity = 0)
train.xgboostS3.score <- train.xgboostS3$results[1,]

```

# 3 Evaluation

```{r}

#tree
score <- train.dt.score
tree <- c(score[1,"F"],score[1,"Precision"],score[1,"Recall"],score[1,"AUC"],score[1,"FSD"],score[1,"PrecisionSD"], score[1,"RecallSD"], score[1,"AUCSD"])

score <- train.dtS1.score
tree_S1 <- c(score[1,"F"],score[1,"Precision"],score[1,"Recall"],score[1,"AUC"],score[1,"FSD"],score[1,"PrecisionSD"], score[1,"RecallSD"], score[1,"AUCSD"])

score <- train.dtS2.score
tree_S2 <- c(score[1,"F"],score[1,"Precision"],score[1,"Recall"],score[1,"AUC"],score[1,"FSD"],score[1,"PrecisionSD"], score[1,"RecallSD"], score[1,"AUCSD"])

score <- train.dtS3.score
tree_S3 <- c(score[1,"F"],score[1,"Precision"],score[1,"Recall"],score[1,"AUC"],score[1,"FSD"],score[1,"PrecisionSD"], score[1,"RecallSD"], score[1,"AUCSD"])

#random forest
score <- train.rf.score
rf <- c(score[1,"F"],score[1,"Precision"],score[1,"Recall"],score[1,"AUC"],score[1,"FSD"],score[1,"PrecisionSD"], score[1,"RecallSD"], score[1,"AUCSD"])

score <- train.rfS1.score
rf_S1 <- c(score[1,"F"],score[1,"Precision"],score[1,"Recall"],score[1,"AUC"],score[1,"FSD"],score[1,"PrecisionSD"], score[1,"RecallSD"], score[1,"AUCSD"])

score <- train.rfS2.score
rf_S2 <- c(score[1,"F"],score[1,"Precision"],score[1,"Recall"],score[1,"AUC"],score[1,"FSD"],score[1,"PrecisionSD"], score[1,"RecallSD"], score[1,"AUCSD"])

score <- train.rfS3.score
rf_S3 <- c(score[1,"F"],score[1,"Precision"],score[1,"Recall"],score[1,"AUC"],score[1,"FSD"],score[1,"PrecisionSD"], score[1,"RecallSD"], score[1,"AUCSD"])

#KNN
score <- train.knn.score
knn <- c(score[1,"F"],score[1,"Precision"],score[1,"Recall"],score[1,"AUC"],score[1,"FSD"],score[1,"PrecisionSD"], score[1,"RecallSD"], score[1,"AUCSD"])

score <- train.knnS1.score
knn_S1 <- c(score[1,"F"],score[1,"Precision"],score[1,"Recall"],score[1,"AUC"],score[1,"FSD"],score[1,"PrecisionSD"], score[1,"RecallSD"], score[1,"AUCSD"])

score <- train.knnS2.score
knn_S2 <- c(score[1,"F"],score[1,"Precision"],score[1,"Recall"],score[1,"AUC"],score[1,"FSD"],score[1,"PrecisionSD"], score[1,"RecallSD"], score[1,"AUCSD"])

score <- train.knnS3.score
knn_S3 <- c(score[1,"F"],score[1,"Precision"],score[1,"Recall"],score[1,"AUC"],score[1,"FSD"],score[1,"PrecisionSD"], score[1,"RecallSD"], score[1,"AUCSD"])

#XGBOOST
score <- train.xgboost.score
xgboost <- c(score[1,"F"],score[1,"Precision"],score[1,"Recall"],score[1,"AUC"],score[1,"FSD"],score[1,"PrecisionSD"], score[1,"RecallSD"], score[1,"AUCSD"])

score <- train.xgboostS1.score
xgboost_S1 <- c(score[1,"F"],score[1,"Precision"],score[1,"Recall"],score[1,"AUC"],score[1,"FSD"],score[1,"PrecisionSD"], score[1,"RecallSD"], score[1,"AUCSD"])

score <- train.xgboostS2.score
xgboost_S2 <- c(score[1,"F"],score[1,"Precision"],score[1,"Recall"],score[1,"AUC"],score[1,"FSD"],score[1,"PrecisionSD"], score[1,"RecallSD"], score[1,"AUCSD"])

score <- train.xgboostS3.score
xgboost_S3 <- c(score[1,"F"],score[1,"Precision"],score[1,"Recall"],score[1,"AUC"],score[1,"FSD"],score[1,"PrecisionSD"], score[1,"RecallSD"], score[1,"AUCSD"])
#LDA
score <- train.lda.score
lda <- c(score[1,"F"],score[1,"Precision"],score[1,"Recall"],score[1,"AUC"],score[1,"FSD"],score[1,"PrecisionSD"], score[1,"RecallSD"], score[1,"AUCSD"])

x <- data.frame(row.names=c("F1", "PREC", "RECALL", "AUC", "F1STD", "PRECSTD", "RECALLSTD", "AUCSTD"),
                Tree = tree, TreeSub1 = tree_S1, TreeSubS2 = tree_S2, TreeSub3 = tree_S3,
                Rf =  rf, RfSub1 = rf_S1, RfSub2 = rf_S2, RfSub3 = rf_S3, 
                KNN = knn, KNNSub1 = knn_S1, KNNSub2 = knn_S2, KNNSub3 = knn_S3, 
                LDA=lda, 
                XGBOOST=xgboost, XGBOOSTSub1 = xgboost_S1, XGBOOSTSub2 = xgboost_S2, XGBOOSTSub3 = xgboost_S3 )

print(x)
barplot(unlist(x["F1",]), names.arg=colnames(x), main="F1", ylim=c(0.7, 1),las = 2, cex.names = 1)
barplot(unlist(x["PREC",]), names.arg=colnames(x), main="Precision",  ylim=c(0.7, 1),las = 2, cex.names = 1)
barplot(unlist(x["RECALL",]), names.arg=colnames(x), main="Recall",  ylim=c(0.7, 1),las = 2, cex.names = 1)
barplot(unlist(x["AUC",]), names.arg=colnames(x), main="AUC",  ylim=c(0.2, 1),las = 2, cex.names = 1)


```

First we compared AUC scores: Random Forest and XGBOOST performed similar. Then we checked F1 score and random forest had slight advantage so we choose it for out final model.

```{r}
p <- predict(train.rf, test)
confusion_matrix <- confusionMatrix(test$Class, p)
p <- predict(train.rf, test, type="prob")
r <- roc(test$Class, p[,1], plot=TRUE)
AUC <- r$auc
precision <- confusion_matrix$table[2,2] / (confusion_matrix$table[2,2] + confusion_matrix$table[2,1])
recall <- confusion_matrix$table[2,2] / (confusion_matrix$table[2,2] + confusion_matrix$table[1,2])
f1_score <- confusion_matrix$F1
AUC
precision
recall
f1_score

```
